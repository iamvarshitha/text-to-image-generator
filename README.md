AI-Powered Text-to-Image Generator

This project is part of my ML Internship assessment for Talrn. The aim was to build a complete text-to-image generation system using open-source models. 
The system takes a text description from the user, processes it through a Stable Diffusion model, and generates images in different styles. 
I built a small web interface so that the application can be used directly in the browser without requiring access to the backend code.

1. Project Overview

The idea behind this project is to understand how modern generative models (especially diffusion models) translate text prompts into images.
My system has four major parts:

User Interface (Streamlit) â€“ where the user types prompts and selects settings
Generator Module â€“ loads the Stable Diffusion model and handles all generation
Utility Layer â€“ watermarking, saving outputs, simple prompt safety checks
Storage â€“ organizes images + metadata in a structured folder

The project focuses on clarity, modularity, and realistic output quality.

2. Architecture

ðŸ§± 2. Project Architecture

```bash
ai-image-generator/
â”œâ”€â”€ app.py                 # User interface built with Streamlit
â”œâ”€â”€ generator.py           # Core model logic for textâ†’image
â”œâ”€â”€ utils.py               # Helper functions (watermark, saving, filtering)
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ outputs/               # Auto-created folders storing images + metadata
```

Flow of the application:

User enters a prompt
Prompt is enriched with style + quality tags
Stable Diffusion generates 1â€“4 images
Images are watermarked and saved
Results are shown in the UI, with download options


